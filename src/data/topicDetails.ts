import { CategoryKey } from './topics';

export type TopicFigure = {
  src: string;     // put images in public/images/... then use "/images/..."
  alt: string;
  caption?: string;
};

export type TopicDetail = {
  category: CategoryKey;
  slug: string;
  title: string;
  intro?: string;
  paragraphs: string[];     // put your 500 words split into paragraphs
  figures?: TopicFigure[];  // optional
};

export const topicDetails: TopicDetail[] = [
  // =========================
  // BIOINFORMATICS (6)
  // =========================

  {
    category: 'bioinformatics',
    slug: 'overview',
    title: 'Overview: What Bioinformatics Covers',
    intro: 'A practical map of the field: data types, workflows, and interpretation.',
    paragraphs: [
      'Bioinformatics sits at the intersection of biology, computer science, and statistics, transforming how we understand living systems through data-driven approaches. At its core, bioinformatics involves the development and application of computational tools to analyze, interpret, and manage biological information. This field has become indispensable in modern biology and medicine, enabling researchers to make sense of massive datasets generated by high-throughput technologies like next-generation sequencing, mass spectrometry, and various imaging modalities.',
      'The data types in bioinformatics are diverse and complex. Genomic data includes DNA sequences that encode an organism\'s complete genetic blueprint, while transcriptomic data captures RNA molecules to reveal gene expression patterns. Proteomic data focuses on proteins, the functional workhorses of cells, including their abundance, modifications, and interactions. Metabolomic data examines small molecules that participate in cellular metabolism. Each data type requires specialized analytical approaches and bioinformatics tools to extract meaningful biological insights.',
      'A typical bioinformatics workflow follows a structured pipeline from raw data generation to biological interpretation. The journey begins with data generation in the laboratory, producing raw files that must undergo quality control to identify and remove artifacts. Next comes preprocessing, where data is cleaned, filtered, and formatted for analysis. The core analysis phase applies algorithms and statistical methods to identify patterns, make comparisons, or generate predictions. Finally, interpretation connects computational results back to biological questions, often requiring integration with existing knowledge databases and literature.',
      'RNA sequencing (RNA-seq) represents one of the most widely used bioinformatics applications, measuring gene expression across different conditions or cell types. The RNA-seq workflow processes raw sequencing reads, aligns them to reference genomes, quantifies gene expression levels, and identifies differentially expressed genes. This approach has revolutionized our understanding of gene regulation, cellular responses to stimuli, and disease mechanisms. Bioinformatics tools like STAR, DESeq2, and edgeR have become standard in most molecular biology laboratories.',
      'Single-cell RNA sequencing extends traditional RNA-seq by analyzing gene expression at the individual cell level, revealing cellular heterogeneity and rare cell populations invisible in bulk analyses. This technology requires sophisticated bioinformatics methods for dimensionality reduction, clustering, and cell type identification. Tools like Seurat, Scanpy, and CellRanger enable researchers to map cellular landscapes, track developmental trajectories, and understand how individual cells contribute to tissue function and disease progression.',
      'Beyond transcriptomics, bioinformatics encompasses variant calling to identify genetic mutations, pathway analysis to understand gene function networks, and structural biology predictions using artificial intelligence. The field continues to evolve rapidly, with machine learning approaches becoming increasingly important for predicting protein structures, identifying biomarkers, and understanding complex biological systems. Successful bioinformatics practitioners must combine technical programming skills with deep biological understanding to translate computational results into actionable scientific insights.',
    ],
    figures: [
      {
        src: '/images/topics/overview/figure1.png',
        alt: 'Bioinformatics workflow diagram',
        caption: 'Example: From raw sequencing reads to biological interpretation.',
      },
    ],
  },

  {
    category: 'bioinformatics',
    slug: 'rna-seq',
    title: 'RNA-seq (Bulk RNA Sequencing)',
    intro: 'End-to-end workflow: FASTQ → QC → quantification → DE → pathways.',
    paragraphs: [
      'Bulk RNA sequencing (RNA-seq) is a widely used method to measure gene expression by sequencing RNA molecules from a biological sample and mapping them back to genes or transcripts. Unlike single-cell RNA-seq, bulk RNA-seq captures an averaged signal across a population of cells, which makes it ideal for comparing conditions such as treated vs untreated, tumor vs normal, or timepoint differences. The goal is not only to list “up” and “down” genes, but to produce a defensible, reproducible summary of how expression changes relate to biology.',
      'The workflow typically begins with raw FASTQ files. The first step is quality control (QC) using tools such as FastQC (and MultiQC for aggregation). QC helps you check base quality across read positions, adapter contamination, duplication levels, overrepresented sequences, and GC distribution. If QC indicates adapter contamination or poor quality tails, trimming tools such as Cutadapt or Trim Galore are used to remove adapters and low-quality bases. A common best practice is to run QC both before and after trimming so you can show the improvement and justify the chosen parameters.',
      'After reads are cleaned, you choose a quantification strategy. One approach is alignment to a reference genome using tools like STAR or HISAT2, which produces BAM files and supports detailed downstream QC (mapping rate, rRNA contamination checks, insert size, gene body coverage). Another approach is fast transcript-level quantification using pseudo-alignment tools like Salmon or Kallisto, which are efficient and widely accepted for expression analysis. Your choice depends on project needs: alignment-based methods are useful when you need read-level inspection; pseudo-alignment is often sufficient for standard DE workflows.',
      'Next, counts are summarized into a gene-level matrix. Because sequencing depth varies across samples, normalization is essential. DESeq2 size-factor normalization or edgeR TMM normalization are common choices. For visualization and unsupervised exploration (PCA, clustering), count transformations like variance-stabilizing transformation (VST) or rlog help reduce mean–variance dependence. At this stage, you also inspect sample-to-sample relationships to detect outliers and batch effects. If batch effects exist, you address them in the model design (preferred) or consider correction approaches when appropriate.',
      'Differential expression (DE) analysis compares conditions using statistical models that account for biological variability and experimental design. The most important part is not clicking “run,” but specifying the correct design: batch covariates, paired samples, and relevant clinical variables should be included when available. Results are typically summarized with log2 fold change and adjusted p-values (FDR). Standard plots include MA plots, volcano plots, and heatmaps of top genes. A good analysis also includes effect-size reasoning, not only significance, because very large sample sizes can make tiny effects statistically significant.',
      'Finally, pathway and gene set interpretation translates gene-level results into biology. Gene set enrichment analysis (GSEA) can capture coordinated shifts in pathways even when individual genes are modestly changed. GO, KEGG, and Reactome are common resources for pathway definitions. In a report, you should connect enriched pathways to the study context (for example, immune activation, cell cycle, or metabolism changes), and state limitations (gene set redundancy, annotation bias, cell composition confounding). A strong RNA-seq deliverable includes QC summaries, mapping/quantification metrics, sample-level PCA, DE outputs with clear thresholds, and pathway interpretation, all captured in a reproducible pipeline (Snakemake/Nextflow) with documented versions and parameters.',
    ],
    figures: [
      {
        src: '/images/topics/rna-seq/figure1.png',
        alt: 'RNA-seq pipeline diagram',
        caption: 'Pipeline: FASTQ → QC → alignment/quantification → counts → DE → pathways.',
      },
      {
        src: '/images/topics/rna-seq/figure2.png',
        alt: 'Volcano plot example',
        caption: 'Example visualization: log2 fold change vs statistical significance for DE genes.',
      },
    ],
  },

  {
    category: 'bioinformatics',
    slug: 'scrna-seq',
    title: 'scRNA-seq (Single-cell RNA Sequencing)',
    intro: 'QC, normalization, clustering, annotation, DE, and integration.',
    paragraphs: [
      'Single-cell RNA sequencing (scRNA-seq) has revolutionized our understanding of cellular heterogeneity by enabling gene expression profiling at the individual cell level. Unlike bulk RNA-seq, which averages signals across thousands to millions of cells, scRNA-seq reveals the diversity of cell types, states, and trajectories within complex tissues. This technology has become essential for developmental biology, immunology, neuroscience, and cancer research, where cellular composition and rare cell populations play crucial roles in biological function and disease progression.',
      'The scRNA-seq workflow begins with careful sample preparation to obtain a viable single-cell suspension while minimizing stress-induced transcriptional changes. Cells are then captured using microfluidic droplets, microwells, or combinatorial indexing strategies, each with distinct advantages in terms of throughput, cost, and cell type compatibility. Following cell capture, mRNA molecules are reverse-transcribed and amplified to generate sequencing libraries. Critical quality control steps include assessing cell viability, doublet rates, and library complexity before proceeding to sequencing and computational analysis.',
      'Quality control forms the foundation of scRNA-seq data analysis, involving multiple steps to ensure data reliability. Initial filtering removes low-quality cells based on metrics like total UMI counts, detected genes, and mitochondrial gene percentage. Doublets (two cells mistakenly captured as one) are identified and removed using computational tools. Gene filtering removes rarely expressed genes that provide little biological information. Visualization of QC metrics through scatter plots and violin plots helps identify outlier cells and determine appropriate filtering thresholds for downstream analysis.',
      'Normalization addresses technical variations between cells arising from differences in capture efficiency, sequencing depth, and amplification bias. Methods like SCTransform, scran, or log-normalization adjust for library size effects while preserving biological variation. Integration techniques such as Harmony, Seurat integration, or Liger correct for batch effects when combining datasets from different experiments or donors. These normalization and integration steps are essential for meaningful comparative analyses and for detecting subtle biological differences across experimental conditions.',
      'Dimensionality reduction transforms high-dimensional gene expression data into lower-dimensional spaces for visualization and analysis. Principal Component Analysis (PCA) identifies major sources of variation, followed by non-linear methods like t-SNE or UMAP for visualization of cell relationships. Clustering algorithms group cells based on transcriptional similarity, revealing distinct cell populations and subpopulations. The resolution of clustering parameters must be carefully chosen to balance over-clustering (splitting true populations) versus under-clustering (merging distinct populations).',
      'Cell type annotation connects computational clusters to biological identities using marker genes, reference datasets, or automated annotation tools. Differential expression analysis identifies genes that distinguish between cell types, conditions, or developmental states. Trajectory inference methods like Monocle, Slingshot, or Pseudotime reconstruct cellular differentiation pathways and dynamic processes. Integration with other data modalities, such as spatial transcriptomics or chromatin accessibility, provides additional layers of biological insight and context for single-cell gene expression patterns.',
    ], 
    figures: [
      {
        src: '/images/topics/scrna-seq/figure1.png',
        alt: 'UMAP example',
        caption: 'UMAP showing clusters of cells after dimensionality reduction.',
      },
    ],
  },

{
    category: 'bioinformatics',
    slug: 'variant-calling',
    title: 'Variant Calling',
    intro: 'Reads → BAM/VCF, filtering, annotation, and interpretation.',
    paragraphs: [
      'Variant calling represents a fundamental bioinformatics process for identifying genetic differences between samples, such as somatic mutations in cancer or germline variants in inherited diseases. This computational workflow transforms raw sequencing data into variant calls that can be interpreted for clinical diagnosis, research insights, or therapeutic decision-making. The accuracy and reliability of variant calling directly impact downstream analyses, making quality control and proper parameter selection essential components of any genomic analysis pipeline.',
      'The variant calling workflow begins with read alignment to a reference genome using tools like BWA-MEM or Bowtie2, producing BAM files that map each sequencing read to its genomic location. Alignment quality metrics, including mapping rate, insert size distribution, and coverage uniformity, must be evaluated before proceeding. Duplicate marking with Picard tools removes PCR artifacts that could inflate variant allele frequencies. Base quality score recalibration (BQSR) adjusts quality scores to improve variant calling accuracy, particularly important for clinical applications where false positives must be minimized.',
      'Variant callers employ different statistical models to distinguish true variants from sequencing errors. GATK HaplotypeCaller uses a local de novo assembly approach, ideal for germline variant detection in high-quality samples. MuTect2 specializes in somatic variant calling, incorporating tumor-normal paired analysis and sophisticated filtering for low allele frequency variants. FreeBayes and VarScan offer alternative approaches with different strengths in handling complex variants or mixed populations. The choice of variant caller depends on sample type, expected variant frequency, and specific research or clinical requirements.',
      'Variant filtering removes false positives while preserving true variants through a multi-step quality assessment process. Hard filters apply thresholds to metrics like depth, quality score, strand bias, and mapping quality. Variant Quality Score Recalibration (VQSR) uses machine learning to model the distribution of true versus false variants based on training sets. Additional filters remove variants in problematic genomic regions, such as segmental duplications or low-complexity sequences. Manual review of variants in genome browsers like IGV provides an additional quality check for clinically relevant variants.',
      'Variant annotation adds biological context to identified variants, transforming raw genomic coordinates into interpretable information. Tools like ANNOVAR, VEP, or SnpEff predict functional consequences (synonymous, missense, nonsense, splice site) and map variants to genes and transcripts. Population frequency databases (gnomAD, 1000 Genomes) help distinguish common polymorphisms from rare potentially pathogenic variants. Clinical databases (ClinVar, HGMD) provide information about known disease associations. Functional prediction scores (SIFT, PolyPhen, CADD) estimate the potential impact of missense variants on protein function.',
      'Interpretation of clinical variants follows established guidelines like ACMG/AMP standards, which classify variants into pathogenic, likely pathogenic, uncertain significance, likely benign, or benign categories. This interpretation integrates population frequency, computational predictions, functional data, segregation information, and phenotype matching. For research applications, variant prioritization focuses on genes relevant to the biological question or disease pathway. Documentation of the variant calling pipeline, including software versions, parameters, and quality metrics, ensures reproducibility and facilitates regulatory compliance for clinical applications.',
    ],
    figures: [
      { src: '/images/topics/variant-calling/figure1.png', alt: 'Variant calling flow', caption: 'Alignment → variant calling → filtering → annotation.' },
    ],
  },

{
    category: 'bioinformatics',
    slug: 'gsea-pathways',
    title: 'GSEA & Pathways',
    intro: 'GO/KEGG/Reactome enrichment and biological interpretation.',
    paragraphs: [
      'Gene Set Enrichment Analysis (GSEA) and pathway analysis transform lists of differentially expressed genes into biological insights by identifying coordinated changes in predefined gene sets. Rather than focusing on individual genes, these methods detect subtle but consistent shifts in biological pathways, cellular processes, or molecular functions. This systems-level approach provides a more comprehensive understanding of biological responses to experimental conditions, disease states, or therapeutic interventions, helping researchers move from gene lists to mechanistic interpretations.',
      'Over-Representation Analysis (ORA) represents the most straightforward pathway enrichment approach, testing whether genes in predefined sets appear more frequently than expected among differentially expressed genes. Methods like Fisher\'s exact test or hypergeometric test evaluate the significance of overlap between gene lists and pathways from databases like Gene Ontology (GO), KEGG, Reactome, or MSigDB. ORA requires setting arbitrary significance thresholds for gene selection, which can introduce bias and miss coordinated but modest changes across pathways. Despite these limitations, ORA remains popular due to its simplicity and intuitive interpretation.',
      'Gene Set Enrichment Analysis (GSEA) overcomes ORA limitations by considering all genes ranked by a statistic like log2 fold change or signal-to-noise ratio. GSEA calculates an enrichment score that reflects whether genes in a pathway tend to appear at the top or bottom of the ranked list, capturing coordinated expression changes even when individual genes don\'t meet significance thresholds. The method uses permutation testing to assess significance and accounts for gene set size correlations. GSEA is particularly valuable for detecting pathway-level changes in complex diseases or subtle treatment effects where individual gene changes are modest.',
      'Pathway databases provide the biological context necessary for enrichment analysis, each with distinct strengths and focus areas. Gene Ontology (GO) offers hierarchical annotations for biological processes, molecular functions, and cellular components, providing broad functional categorization. KEGG maps genes to metabolic and signaling pathways with detailed pathway diagrams and organism-specific information. Reactome emphasizes manually curated pathways with detailed reaction mechanisms and molecular interactions. MSigDB contains curated gene sets for transcription factor targets, hallmarks, and disease signatures, offering specialized collections for different research questions.',
      'Visualization tools help interpret enrichment results and communicate findings effectively. Enrichment maps display relationships between enriched pathways as networks, where node size represents significance and edge thickness indicates gene overlap. Bubble plots show pathway significance versus gene ratio, with color coding for adjusted p-values. Ridge plots or dot plots illustrate expression patterns of genes within significant pathways. For GSEA, enrichment plots show the running enrichment score across ranked genes, highlighting where pathway members contribute most to the signal. These visualizations make complex pathway relationships accessible to diverse audiences.',
      'Advanced pathway analysis methods incorporate additional biological layers beyond simple gene lists. Pathway topology analysis considers the position and interaction of genes within pathways, giving more weight to upstream regulators or hub genes. Network-based methods integrate protein-protein interactions, co-expression relationships, or regulatory networks to identify key drivers of pathway changes. Multi-omics pathway analysis combines transcriptomics with proteomics, metabolomics, or epigenomics to provide a more comprehensive view of pathway activity. These sophisticated approaches require careful interpretation but can reveal deeper mechanistic insights than standard enrichment methods.',
    ],
    figures: [
      { src: '/images/topics/gsea-pathways/figure1.png', alt: 'Enrichment plot', caption: 'Example: running enrichment score over ranked genes.' },
    ],
  },

{
    category: 'bioinformatics',
    slug: 'pipelines',
    title: 'Pipelines (Reproducible Workflows)',
    intro: 'Snakemake/Nextflow, containers, HPC, and best practices.',
    paragraphs: [
      'Bioinformatics pipelines transform complex multi-step analyses into reproducible, automated workflows that can be shared, validated, and executed across different computing environments. These workflow management systems encode data processing steps, dependencies, and computational requirements in declarative formats that enable consistent analysis regardless of who runs the pipeline or where it\'s executed. Modern bioinformatics relies heavily on pipeline frameworks like Snakemake, Nextflow, and WDL to ensure reproducibility, scalability, and transparency in data analysis, particularly for large-scale projects involving multiple samples and analytical steps.',
      'Snakemake uses Python-based rules to define workflow steps, with automatic dependency resolution based on input/output file patterns. Each rule specifies required inputs, expected outputs, and the command or script to execute, allowing Snakemake to construct a directed acyclic graph (DAG) and determine optimal execution order. The framework supports conda environments for software management, cluster integration for HPC systems, and checkpoint mechanisms for dynamic workflow branching. Snakemake\'s syntax is particularly accessible for users familiar with Python, making it popular in academic bioinformatics labs and research environments.',
      'Nextflow offers a domain-specific language for pipeline definition with strong support for containerization and cloud execution. Its process-channel architecture separates computational steps (processes) from data flow (channels), enabling flexible pipeline design and efficient resource utilization. Nextflow excels at cross-platform deployment, seamlessly transitioning between local workstations, HPC clusters, and cloud providers like AWS, Google Cloud, or Azure. The framework\'s built-in support for Docker, Singularity, and Conda containers ensures software consistency across environments, while its automatic parallelization and resource management optimize computational efficiency.',
      'Container technologies like Docker and Singularity provide isolated, reproducible software environments that encapsulate all dependencies required for bioinformatics tools. Docker containers work well for cloud and local development, while Singularity addresses HPC security requirements by running containers without root privileges. Container registries (Docker Hub, Biocontainers) host pre-built images for common bioinformatics software, eliminating installation complexities. Containerization solves the "works on my machine" problem by ensuring that pipeline execution environments are identical across development, testing, and production systems, which is essential for clinical and regulatory applications.',
      'High-Performance Computing (HPC) integration enables pipelines to leverage computational clusters for large-scale analyses. Workflow managers interface with job schedulers like SLURM, PBS, or SGE to distribute tasks across multiple nodes, automatically handling job submission, monitoring, and resource allocation. Cloud computing platforms offer elastic scaling with pay-as-you-go pricing, allowing pipelines to dynamically provision resources based on workload demands. Both HPC and cloud execution require careful consideration of data transfer costs, storage requirements, and computational efficiency to optimize performance and minimize expenses.',
      'Pipeline best practices emphasize version control, comprehensive documentation, and thorough testing. Git repositories track pipeline code changes, enabling collaboration and rollback capabilities. Configuration files separate pipeline logic from analysis parameters, allowing easy adaptation to different projects or datasets. Automated testing with synthetic or benchmark datasets validates pipeline correctness and detects regressions. Metadata capture, quality metrics, and provenance tracking ensure that pipeline results are auditable and reproducible. Following these practices creates robust, maintainable pipelines that can be shared with the broader scientific community and adapted for future research needs.',
    ],
    figures: [
      { src: '/images/topics/pipelines/figure1.png', alt: 'Pipeline DAG', caption: 'Example: a workflow DAG with steps and dependencies.' },
    ],
  },

  // =========================
  // BIOTECHNOLOGY (4)
  // =========================
{
    category: 'biotechnology',
    slug: 'ngs-library-prep',
    title: 'NGS Library Prep',
    intro: 'Fragmentation, adapters, PCR, QC, and pitfalls.',
    paragraphs: [
      'Next-generation sequencing library preparation transforms nucleic acids into sequencing-compatible formats through a series of carefully controlled biochemical steps. This critical process determines data quality, library complexity, and ultimately the success of sequencing projects. Library preparation involves fragmenting nucleic acids to appropriate sizes, adding sequencing adapters, amplifying library molecules, and performing rigorous quality control. Each step requires optimization for specific sample types, sequencing platforms, and experimental goals, making library preparation both an art and a science in molecular biology laboratories.',
      'Fragmentation methods generate appropriately sized DNA or RNA fragments for sequencing, typically targeting 200-500 base pairs for Illumina platforms. Mechanical shearing uses sonication (Covaris) or nebulization to break DNA through physical force, producing random fragments with minimal sequence bias. Enzymatic fragmentation employs transposases (Nextera) or restriction enzymes to simultaneously fragment and add adapter sequences, offering faster workflows but potentially introducing sequence bias. RNA fragmentation often uses chemical methods (heat with divalent cations) or enzymatic approaches, with the choice depending on RNA quality and downstream applications like RNA-seq or small RNA sequencing.',
      'Adapter ligation attaches platform-specific sequences to fragment ends, enabling cluster generation and sequencing primer binding. Illumina adapters contain flow cell binding sequences, sequencing primer sites, and unique molecular identifiers (UMIs) for duplicate removal. Ligation-based methods use T4 DNA ligase to join double-stranded adapters to fragment ends, requiring careful optimization of adapter:insert ratios to minimize adapter dimer formation. Tagmentation-based approaches (Nextera) use transposases to insert adapters during fragmentation, reducing hands-on time but requiring careful control of enzyme activity to achieve optimal fragment size distribution.',
      'PCR amplification enriches for adapter-ligated fragments and adds necessary index sequences for multiplexing. Limited-cycle PCR (typically 8-12 cycles) balances library yield with the risk of amplification bias and duplicate formation. High-fidelity polymerases minimize introduction of errors during amplification, which is crucial for applications like variant calling. Index sequences (barcodes) enable pooling of multiple libraries in a single sequencing run, with unique dual indexing reducing index hopping and sample cross-contamination. PCR-free library preparation eliminates amplification bias but requires higher input quantities and may have lower success rates with degraded samples.',
      'Quality control assesses library concentration, fragment size distribution, and adapter dimer contamination before sequencing. Fluorometric methods (Qubit) accurately quantify double-stranded DNA concentration, while spectrophotometry (NanoDrop) assesses purity through A260/280 and A260/230 ratios. Fragment analysis (Bioanalyzer, TapeStation) determines size distribution and identifies adapter dimers that can interfere with cluster generation. qPCR-based quantification (KAPA Library Quantification) provides the most accurate measurement of amplifiable library molecules, essential for optimal cluster density and sequencing performance.',
      'Common library preparation pitfalls include adapter dimer formation, over-amplification, size selection errors, and contamination. Adapter dimers compete with library fragments for cluster generation, reducing sequencing yield and data quality. Over-amplification introduces bias and duplicates, particularly problematic for quantitative applications like RNA-seq. Inaccurate size selection leads to suboptimal cluster formation or poor read quality. Contamination from previous libraries or environmental DNA can compromise results, requiring strict cleanroom practices and no-template controls. Careful optimization, appropriate controls, and thorough QC at each step ensure successful library preparation and high-quality sequencing data.',
    ],
    figures: [{ src: '/images/topics/ngs-library-prep/figure1.png', alt: 'Library prep steps', caption: 'High-level steps in NGS library preparation.' }],
  },
{
    category: 'biotechnology',
    slug: 'pcr-qpcr',
    title: 'PCR & qPCR',
    intro: 'Ct values, standard curves, and troubleshooting.',
    paragraphs: [
      'Polymerase Chain Reaction (PCR) and quantitative PCR (qPCR) represent fundamental molecular biology techniques for amplifying and quantifying nucleic acids. PCR enables exponential amplification of specific DNA sequences through repeated temperature cycles of denaturation, annealing, and extension. qPCR extends this capability by monitoring amplification in real-time using fluorescent reporters, allowing precise quantification of nucleic acid targets. These techniques underpin applications ranging from gene expression analysis and pathogen detection to clone verification and diagnostic testing across research and clinical laboratories.',
      'PCR fundamentals rely on the thermostable Taq polymerase enzyme, which synthesizes new DNA strands during the extension step. Each PCR cycle theoretically doubles the target DNA amount, producing exponential amplification that can generate millions of copies from a single starting molecule. Key reaction components include DNA template, forward and reverse primers that flank the target region, deoxynucleotide triphosphates (dNTPs), magnesium ions as enzyme cofactors, and buffer solution. Optimizing primer design, annealing temperature, and cycle number is crucial for specific, efficient amplification without non-specific products or primer-dimer formation.',
      'qPCR detection methods include SYBR Green fluorescence and probe-based chemistries like TaqMan. SYBR Green intercalates into double-stranded DNA, providing simple, cost-effective detection but potentially binding to non-specific products. TaqMan probes use sequence-specific oligonucleotides labeled with reporter and quencher fluorophores, providing increased specificity through dual recognition. Alternative chemistries include Molecular Beacons, Scorpion probes, and EvaGreen dye, each offering distinct advantages for different applications. The choice of detection chemistry depends on required specificity, multiplexing capability, and budget considerations.',
      'Ct (threshold cycle) values represent the PCR cycle at which fluorescence crosses a predefined threshold, inversely correlating with starting template quantity. Lower Ct values indicate higher initial template amounts, while higher values suggest lower starting concentrations. Ct values typically range from 15-35 cycles in reliable assays, with values above 35 often indicating low template abundance or potential contamination. Consistent threshold setting across experiments ensures comparable Ct measurements, while triplicate technical replicates provide measurement precision and identify outliers. Understanding Ct interpretation is essential for accurate quantification and experimental decision-making.',
      'Standard curves enable absolute quantification by relating Ct values to known template concentrations through serial dilutions. Ideal standard curves demonstrate 90-110% amplification efficiency, calculated from the slope of the regression line (E = 10^(-1/slope) - 1). The correlation coefficient (R²) should exceed 0.99 for reliable quantification. Standard curves must include the dynamic range of expected sample concentrations and use the same matrix as unknown samples to account for potential inhibitors. Regular standard curve validation ensures assay performance consistency and quantification accuracy across different runs and operators.',
      'PCR and qPCR troubleshooting addresses common issues including no amplification, weak signals, non-specific products, and inconsistent results. No amplification often stems from incorrect annealing temperature, degraded reagents, or template absence. Weak signals may result from suboptimal primer design, inhibitors, or insufficient template. Non-specific products manifest as multiple bands in conventional PCR or abnormal amplification curves in qPCR. Inconsistent results typically indicate pipetting errors, inadequate mixing, or thermal cycler performance issues. Systematic troubleshooting guides incorporating reaction component checks, gradient PCR optimization, and contamination control help identify and resolve these problems efficiently.',
    ],
    figures: [{ src: '/images/topics/pcr-qpcr/figure1.png', alt: 'qPCR curve', caption: 'Example amplification curve and threshold (Ct).' }],
  },
{
    category: 'biotechnology',
    slug: 'cell-culture',
    title: 'Cell Culture',
    intro: 'Passaging, contamination control, viability, and documentation.',
    paragraphs: [
      'Cell culture provides a controlled environment for growing and maintaining cells outside their natural biological context, enabling researchers to study cellular processes, drug responses, and disease mechanisms. This fundamental technique supports diverse applications from basic biology to drug discovery and biomanufacturing. Successful cell culture requires mastery of sterile technique, understanding of cell-specific requirements, and rigorous quality control. Whether working with adherent cell lines, suspension cultures, or primary cells, consistent practices and detailed documentation ensure reproducible results and experimental integrity.',
      'Cell culture media provides essential nutrients, growth factors, and environmental conditions for cellular growth and maintenance. Basal media formulations like DMEM, RPMI, or MEM supply vitamins, amino acids, and salts, while supplements including fetal bovine serum (FBS), antibiotics, and specific growth factors address specialized requirements. Media selection depends on cell type, growth characteristics, and experimental objectives. Serum-free formulations reduce variability for quantitative applications, while specialized media support specific differentiation pathways. Regular media changes prevent nutrient depletion and waste accumulation, with change frequency optimized for cell growth rate and metabolic activity.',
      'Passaging (subculturing) prevents overconfluence and maintains cells in exponential growth phase through regular dilution into fresh vessels. Adherent cells require detachment using trypsin-EDTA or enzyme-free dissociation solutions, with careful optimization to minimize stress while ensuring complete dissociation. Suspension cultures typically involve direct dilution or centrifugation-based concentration before reseeding. Split ratios (1:2, 1:4, etc.) depend on growth rate and desired seeding density, typically ranging from 10,000-100,000 cells/cm² for adherent lines. Consistent passaging schedules prevent spontaneous differentiation and maintain phenotype stability across experimental replicates.',
      'Contamination control represents the most critical aspect of cell culture, with bacterial, fungal, and mycoplasma infections potentially compromising experiments and wasting resources. Aseptic technique includes working in biosafety cabinets, regular disinfection, proper media handling, and careful pipetting practices. Antibiotic/antimycotic cocktails provide protection but cannot replace good sterile practices. Regular testing for mycoplasma (PCR, staining, or culture methods) should occur monthly or when suspicious symptoms appear. Cross-contamination between cell lines remains common, necessitating authentication through STR profiling, especially for long-term or clinically relevant cultures.',
      'Viability assessment ensures cell health before experimental use and monitors culture conditions over time. Trypan blue exclusion provides rapid viability estimates through dye uptake by dead cells with compromised membranes. Automated cell counters improve consistency and reduce subjectivity compared to manual hemocytometer counting. Metabolic assays like MTT, XTT, or resazurin reduction offer indirect viability measurements based on cellular enzymatic activity. Flow cytometry with propidium iodide or annexin V staining provides detailed viability and apoptosis information for critical applications. Regular viability monitoring helps identify suboptimal conditions before experimental failure.',
      'Documentation maintains experimental reproducibility and supports quality control in cell culture operations. Comprehensive records include cell line source, authentication results, passage number, media formulations, and experimental observations. Digital imaging systems document cellular morphology and confluence at key timepoints. Cryopreservation records track freezer inventory, vial locations, and freeze-thaw cycles. Quality metrics including doubling times, viability percentages, and contamination status create performance baselines for early problem detection. This documentation becomes essential for publication, regulatory compliance, and troubleshooting unexpected experimental outcomes.',
    ],
    figures: [{ src: '/images/topics/cell-culture/figure1.png', alt: 'Cell culture workflow', caption: 'Core steps and checks in cell culture.' }],
  },
{
    category: 'biotechnology',
    slug: 'assays',
    title: 'Assays (ELISA, Western, Flow basics)',
    intro: 'Assay QC mindset and interpretation.',
    paragraphs: [
      'Biochemical assays provide quantitative and qualitative measurements of cellular components, enabling researchers to study protein expression, signaling pathways, and cellular responses. ELISA, Western blotting, and flow cytometry represent fundamental techniques for detecting and quantifying specific molecules in complex biological samples. Each assay offers distinct advantages in sensitivity, throughput, and information content, with selection depending on experimental goals, sample availability, and required precision. Understanding assay principles, limitations, and quality control requirements ensures reliable data generation and meaningful biological interpretation.',
      'ELISA (Enzyme-Linked Immunosorbent Assay) enables sensitive quantification of specific proteins, antibodies, or small molecules in liquid samples using antibody-antigen recognition coupled with enzymatic signal amplification. Direct ELISA uses labeled primary antibodies for rapid detection, while indirect ELISA employs labeled secondary antibodies for signal amplification. Sandwich ELISA provides highest specificity through capture and detection antibodies binding different epitopes, while competitive ELISA quantifies small molecules through displacement assays. Plate-based format enables high-throughput screening with detection limits typically in the picogram to nanogram range, making ELISA ideal for biomarker quantification and therapeutic monitoring.',
      'Western blotting combines electrophoretic protein separation with immunodetection, enabling analysis of protein size, expression level, and post-translational modifications. SDS-PAGE resolves proteins by molecular weight, followed by transfer to nitrocellulose or PVDF membranes for antibody probing. Blocking steps prevent non-specific binding, while primary antibodies provide target specificity and secondary antibodies offer signal amplification. Chemiluminescent, fluorescent, or colorimetric detection enables visualization and quantification using imaging systems. Proper loading controls (housekeeping proteins) and molecular weight markers ensure accurate interpretation and normalization across samples.',
      'Flow cytometry analyzes individual cells in suspension, measuring physical characteristics and fluorescence markers to quantify cellular populations and protein expression. Forward scatter (FSC) correlates with cell size, while side scatter (SSC) reflects internal complexity or granularity. Fluorescent antibodies detect specific surface or intracellular markers, enabling multi-parameter analysis of cellular phenotypes. Modern flow cytometers can analyze 10+ parameters simultaneously, supporting detailed immunophenotyping and cell cycle analysis. Proper compensation controls address spectral overlap between fluorophores, while viability dyes distinguish live from dead cells for accurate population assessment.',
      'Assay validation establishes performance characteristics including sensitivity, specificity, accuracy, precision, linearity, and range. Limit of detection (LOD) and limit of quantification (LOQ) define the smallest measurable amounts with acceptable reliability. Dynamic range encompasses concentrations where assay response remains linear and proportional. Intra-assay precision (repeatability) measures consistency within runs, while inter-assay precision (reproducibility) assesses variation between runs or operators. Specificity testing confirms that assay signals derive from target analytes rather than cross-reactive substances, particularly important for complex biological matrices.',
      'Quality control integrates standards, controls, and reference materials to ensure assay reliability and data integrity. Positive controls confirm assay functionality, while negative controls detect background or non-specific signals. Calibration standards establish quantitative relationships between signal and concentration across the analytical range. Reference materials with known concentrations enable inter-assay comparability and method validation. Control charts track assay performance over time, identifying trends or shifts that may require corrective action. This systematic QC approach supports regulatory compliance, experimental reproducibility, and confidence in biological conclusions derived from assay data.',
    ],
    figures: [{ src: '/images/topics/assays/figure1.png', alt: 'Assay overview', caption: 'Example: assay controls and expected outcomes.' }],
  },

  // =========================
  // BIOPHARMA (4)
  // =========================
{
    category: 'biopharma',
    slug: 'drug-discovery',
    title: 'Drug Discovery',
    intro: 'Targets → leads → optimization → preclinical.',
    paragraphs: [
      'Drug discovery represents a complex, multidisciplinary process that transforms scientific insights into therapeutic candidates through target identification, lead discovery, optimization, and preclinical development. This journey typically spans 5-10 years and costs billions of dollars, with high attrition rates at each stage. Modern drug discovery integrates computational approaches, high-throughput screening, and advanced biology technologies to identify and optimize molecules that can safely and effectively modulate disease-related targets. Understanding the entire discovery pipeline provides essential context for researchers contributing to therapeutic development.',
      'Target identification and validation establish the biological foundation for drug discovery by selecting molecules whose modulation will produce therapeutic effects. Genetic approaches (GWAS, CRISPR screens), proteomics, and disease biology research identify proteins, RNA molecules, or pathways involved in disease pathology. Target validation confirms that modulating the target produces the desired biological effect without unacceptable consequences through genetic knockdown/knockout studies, pharmacological tools, and animal disease models. Validated targets must demonstrate "druggability" - the ability to bind small molecules or biologics with appropriate affinity, specificity, and pharmacological properties to enable therapeutic modulation.',
      'Lead discovery identifies initial compounds that interact with the validated target through high-throughput screening (HTS), fragment-based drug discovery, or rational design approaches. HTS tests hundreds of thousands to millions of compounds against target assays, measuring binding, enzymatic inhibition, or cellular activity. Fragment screening identifies smaller molecular fragments that bind to target pockets, which can be linked and expanded to create higher-affinity leads. Structure-based drug design uses target protein structures to rationally design molecules that fit specific binding pockets, while computational approaches leverage machine learning and molecular modeling to predict promising candidates from virtual libraries.',
      'Lead optimization transforms initial hits into drug candidates through iterative chemical modifications improving potency, selectivity, pharmacokinetic properties, and safety profiles. Medicinal chemists systematically modify molecular structures based on structure-activity relationships (SAR), balancing multiple parameters including target affinity, solubility, metabolic stability, and membrane permeability. ADMET (absorption, distribution, metabolism, excretion, toxicity) profiling identifies potential liabilities early in optimization. Computational tools predict molecular properties and guide synthetic efforts, while in vitro and in vivo assays evaluate pharmacokinetic behavior, target engagement, and preliminary safety parameters.',
      'Preclinical development evaluates optimized lead compounds in comprehensive biological systems to assess therapeutic potential and safety before human testing. In vitro pharmacology studies confirm target engagement and mechanism of action in disease-relevant cellular models. ADME studies in animals evaluate absorption, distribution, metabolism, and elimination to predict human pharmacokinetics. Toxicology testing in multiple species identifies potential safety concerns and establishes no-observed-adverse-effect levels (NOAELs) for clinical trial design. Formulation development creates suitable dosage forms for administration, while scale-up synthesis processes produce sufficient quantities for preclinical and early clinical studies.',
      'Go/No-Go decision points throughout drug discovery determine whether candidates advance to subsequent stages or are terminated based on predefined criteria. Portfolio management balances risk and potential across multiple programs, considering market opportunity, scientific rationale, and competitive landscape. Regulatory interactions with agencies like the FDA provide guidance on preclinical requirements and clinical trial design. Successful transition to clinical development requires comprehensive data packages demonstrating target validation, lead compound properties, efficacy in disease models, and acceptable safety profiles to justify human testing and protect patient welfare.',
    ],
    figures: [{ src: '/images/topics/drug-discovery/figure1.png', alt: 'Drug discovery stages', caption: 'High-level overview of discovery stages.' }],
  },
{
    category: 'biopharma',
    slug: 'clinical-basics',
    title: 'Clinical Basics',
    intro: 'Phases, endpoints, safety vs efficacy.',
    paragraphs: [
      'Clinical trials represent the critical bridge between preclinical drug discovery and approved therapeutic products, systematically evaluating safety and efficacy in human subjects. This highly regulated process progresses through defined phases, each addressing specific questions while protecting participant welfare through ethical guidelines and oversight. Modern clinical development integrates statistical rigor, ethical considerations, and regulatory requirements to generate high-quality evidence supporting therapeutic use. Understanding clinical trial fundamentals enables researchers to design better studies, interpret results appropriately, and contribute to drug development success.',
      'Phase I trials conduct initial human testing in small cohorts (20-100 healthy volunteers or patients) to assess safety, tolerability, pharmacokinetics, and pharmacodynamics of investigational products. Single ascending dose (SAD) studies gradually increase doses in separate groups to determine maximum tolerated dose (MTD) and dose-limiting toxicities (DLTs). Multiple ascending dose (MAD) studies evaluate repeated dosing effects and accumulation profiles. Food effect studies examine how meals impact drug absorption, while special population studies (elderly, renal/hepatic impairment) assess dose adjustments. Intensive safety monitoring includes vital signs, laboratory tests, ECG monitoring, and adverse event recording throughout participation.',
      'Phase II trials explore efficacy and optimal dosing in larger patient populations (100-500) affected by the target disease. Proof-of-concept studies confirm biological activity and preliminary therapeutic benefit, while dose-ranging studies identify optimal dosing regimens balancing efficacy and tolerability. Randomized controlled trials (RCTs) compare investigational products against placebo or standard treatments, employing blinding to prevent bias. Primary endpoints measure key efficacy outcomes (response rate, progression-free survival), while secondary endpoints capture additional benefits and quality-of-life impacts. Adaptive trial designs allow modifications based on interim analyses, potentially accelerating development timelines.',
      'Phase III trials conduct definitive efficacy and safety evaluation in large patient populations (500-5000+) across multiple clinical sites worldwide. These pivotal studies provide the primary evidence supporting regulatory approval and marketing authorization. Superiority trials demonstrate that investigational products outperform standard therapies, while non-inferiority trials establish comparable efficacy with improved safety or convenience. Subgroup analyses examine treatment effects across different patient characteristics, while long-term safety assessment captures rare adverse events not observed in earlier phases. Statistical powering ensures adequate sample size to detect clinically meaningful differences with appropriate confidence.',
      'Phase IV (post-marketing) studies monitor real-world safety and effectiveness after regulatory approval, collecting data from thousands to millions of patients using the therapeutic in routine clinical practice. Pharmacovigilance systems detect rare adverse events, drug-drug interactions, and long-term safety signals not identified during controlled trials. Observational studies using electronic health records, claims databases, and patient registries provide effectiveness evidence across diverse populations and practice settings. Risk Evaluation and Mitigation Strategies (REMS) may be required for products with significant safety concerns. Post-marketing requirements (PMRs) and commitments (PMCs) may be mandated by regulators to address specific outstanding questions.',
      'Clinical trial endpoints measure therapeutic benefit and safety, categorized as primary (key hypothesis), secondary (additional questions), and exploratory (hypothesis-generating) outcomes. Clinical endpoints directly measure how patients feel, function, or survive (overall survival, progression-free survival, symptom improvement). Surrogate endpoints substitute for direct clinical measures when validated biomarkers predict clinical benefit (blood pressure, viral load, tumor response). Safety endpoints capture adverse events, laboratory abnormalities, and treatment discontinuations. Composite endpoints combine multiple outcomes to increase event rates and reduce sample size requirements, particularly important for rare diseases or slowly progressing conditions.',
    ],
    figures: [{ src: '/images/topics/clinical-basics/figure1.png', alt: 'Clinical trial phases', caption: 'Phase I–IV overview.' }],
  },
{
    category: 'biopharma',
    slug: 'biologics',
    title: 'Biologics',
    intro: 'mAbs, immunogenicity, characterization.',
    paragraphs: [
      'Biologics represent a rapidly expanding class of therapeutic products derived from living organisms or their components, including monoclonal antibodies, recombinant proteins, vaccines, and cell-based therapies. These complex molecules offer targeted therapeutic approaches with high specificity and favorable safety profiles compared to traditional small-molecule drugs. Biologics development requires specialized expertise in protein engineering, cell culture manufacturing, and analytical characterization to ensure product consistency, purity, and potency. Their unique properties challenge conventional drug development paradigms, requiring adapted regulatory frameworks and manufacturing approaches.',
      'Monoclonal antibodies (mAbs) dominate the biologics landscape, leveraging the immune system\'s natural ability to recognize specific molecular targets with high affinity. Therapeutic antibodies can neutralize pathogens, block receptor-ligand interactions, modulate immune responses, or deliver cytotoxic payloads to target cells. Humanization techniques reduce immunogenicity by grafting complementarity-determining regions (CDRs) from murine antibodies onto human frameworks. Engineering advances enable Fc region modifications to enhance antibody-dependent cellular cytotoxicity (ADCC), complement activation, or half-life extension. Bispecific antibodies simultaneously target two antigens, enabling novel mechanisms like T-cell redirection for cancer immunotherapy.',
      'Immunogenicity represents the primary safety concern for biologics, as these protein-based therapeutics may trigger unwanted immune responses including anti-drug antibodies (ADAs) that can neutralize therapeutic effects or cause adverse reactions. Immunogenic potential depends on multiple factors including sequence similarity to human proteins, aggregation state, formulation excipients, and patient-specific factors. Risk assessment strategies combine in silico T-cell epitope prediction, in vitro immune cell assays, and animal models to identify immunogenic liabilities early in development. Immunogenicity monitoring in clinical trials measures ADA incidence, titer, and neutralizing capacity, correlating these responses with efficacy and safety outcomes.',
      'Biologics characterization employs sophisticated analytical techniques to define critical quality attributes (CQAs) that ensure product consistency and performance. Primary structure analysis confirms amino acid sequence and post-translational modifications through mass spectrometry and peptide mapping. Higher-order structure assessment determines secondary, tertiary, and quaternary structures using circular dichroism, NMR, and X-ray crystallography. Glycosylation profiling evaluates carbohydrate heterogeneity using LC-MS and HPLC methods, as these modifications significantly impact efficacy and safety. Potency assays measure biological activity through receptor binding, enzymatic activity, or cell-based functional responses, ensuring that structural complexity translates to therapeutic performance.',
      'Biomanufacturing relies on recombinant cell culture systems, primarily Chinese hamster ovary (CHO) cells, to produce complex proteins with appropriate folding, disulfide bonds, and glycosylation patterns. Upstream process optimization maximizes titer while maintaining product quality through media formulation, feeding strategies, and environmental parameter control. Downstream purification uses chromatography techniques including protein A affinity, ion exchange, and size exclusion chromatography to achieve high purity levels. Process analytical technology (PAT) enables real-time monitoring of critical process parameters and quality attributes, facilitating consistent production across manufacturing sites and scales.',
      'Biosimilars provide alternative versions of approved biologics after patent expiration, offering potential cost savings while maintaining therapeutic equivalence. Unlike generic small-molecule drugs, biologics\' complexity prevents exact replication, requiring comprehensive analytical and clinical comparability studies to demonstrate similarity. Stepwise development programs compare structural characteristics, functional activities, pharmacokinetics, pharmacodynamics, and immunogenicity profiles against reference products. Extrapolation of clinical data across indications requires scientific justification based on mechanism of action and immunogenicity risk assessment. Regulatory pathways for biosimilars continue evolving to balance access incentives with rigorous quality standards.',
    ],
    figures: [{ src: '/images/topics/biologics/figure1.png', alt: 'Monoclonal antibody', caption: 'Example structure/regions of an antibody.' }],
  },
{
    category: 'biopharma',
    slug: 'biomarkers',
    title: 'Biomarkers',
    intro: 'Discovery to validation and use cases.',
    paragraphs: [
      'Biomarkers represent measurable indicators of biological processes, pathological states, or pharmacological responses that enable objective assessment of normal or disease-related conditions. These molecular, cellular, or imaging characteristics support diverse applications including disease diagnosis, prognosis, treatment selection, and therapeutic monitoring. Modern biomarker development integrates genomics, proteomics, metabolomics, and advanced imaging technologies to discover and validate indicators that can transform medical practice. Rigorous analytical and clinical validation ensures biomarkers provide reliable, reproducible information that meaningfully impacts clinical decision-making and patient outcomes.',
      'Biomarker discovery employs high-throughput technologies to identify candidate indicators differentiating normal from disease states or predicting treatment responses. Genomic approaches discover genetic variants (SNPs, mutations) associated with disease susceptibility or progression through genome-wide association studies (GWAS) and next-generation sequencing. Transcriptomics identifies expression patterns correlating with disease phenotypes or therapeutic response using RNA-seq and microarray technologies. Proteomics and metabolomics detect protein or metabolite signatures in biological fluids using mass spectrometry and chromatography. Bioinformatic analysis integrates multi-omics data to identify robust, reproducible biomarker candidates with biological relevance and clinical utility.',
      'Analytical validation establishes that biomarker measurements are accurate, precise, and reproducible across laboratories and time periods. Assay development must demonstrate specificity (measuring intended analyte without cross-reactivity), sensitivity (detecting clinically relevant concentrations), linearity (proportional response across concentration range), and robustness (consistent performance despite minor variations). Reference standards and calibration materials ensure measurement traceability and inter-laboratory comparability. Quality control procedures monitor assay performance using internal controls and proficiency testing. Validation requirements depend on intended use, with companion diagnostics requiring higher standards than exploratory research applications.',
      'Clinical validation demonstrates that biomarkers reliably identify the biological condition or predict clinical outcomes in the intended patient population. Retrospective studies use archived clinical samples to assess biomarker performance against known clinical endpoints, providing initial evidence of utility. Prospective observational studies evaluate biomarker performance in real-time clinical settings, establishing predictive values, sensitivity, specificity, and likelihood ratios. Randomized controlled trials (RCTs) provide the highest evidence level, demonstrating that biomarker-guided treatment improves patient outcomes compared to standard approaches. Clinical validation must address potential confounding factors including comorbidities, concomitant medications, and demographic variables.',
      'Regulatory classification and clinical implementation pathways vary based on biomarker application and risk level. Risk stratification biomarkers categorize patients by disease likelihood or progression probability without directly guiding treatment decisions. Predictive biomarkers identify patients likely to benefit from specific therapies, enabling personalized treatment approaches. Pharmacodynamic biomarkers measure target engagement or biological response to therapy, supporting dose optimization and treatment monitoring. Companion diagnostics receive co-approval with corresponding therapeutics, requiring analytical and clinical validation demonstrating essential utility for safe and effective drug use.',
      'Clinical implementation translates validated biomarkers into routine practice through guideline incorporation, reimbursement decisions, and healthcare provider education. Clinical practice guidelines integrate biomarker recommendations with disease management algorithms based on systematic evidence review. Payer coverage decisions consider clinical utility, cost-effectiveness, and alternative diagnostic approaches. Laboratory implementation includes assay standardization, quality management systems, and result reporting formats that support clinical interpretation. Education programs ensure healthcare providers understand biomarker applications, limitations, and appropriate use scenarios. Post-implementation monitoring assesses real-world performance, identifies unexpected issues, and guides refinement of biomarker utilization strategies.',
    ],
    figures: [{ src: '/images/topics/biomarkers/figure1.png', alt: 'Biomarker lifecycle', caption: 'Discovery → validation → clinical utility.' }],
  },

  // =========================
  // QC (4)
  // =========================
{
    category: 'qc',
    slug: 'gmp-glp',
    title: 'GMP / GLP',
    intro: 'Core principles and why compliance matters.',
    paragraphs: [
      'Good Manufacturing Practice (GMP) and Good Laboratory Practice (GLP) represent fundamental quality systems ensuring product safety, efficacy, and consistency in regulated industries. These complementary frameworks establish rigorous standards for production processes, laboratory operations, and quality control throughout product development and manufacturing lifecycles. GMP focuses on consistent production and control of pharmaceutical products, while GLP addresses non-clinical laboratory safety studies. Both systems emphasize systematic approaches, detailed documentation, and quality oversight to prevent errors, ensure reproducibility, and protect public health through regulatory compliance.',
      'GMP principles encompass comprehensive quality management systems controlling all aspects of pharmaceutical production. Quality risk management identifies potential hazards and implements appropriate controls throughout manufacturing processes. Facility and equipment qualification ensures that production environments and instruments meet predefined specifications and consistently perform within acceptable parameters. Personnel training programs demonstrate competency and understanding of GMP requirements through documented assessments and periodic re-evaluations. Process validation demonstrates that manufacturing processes consistently produce products meeting predetermined quality specifications, while cleaning validation prevents cross-contamination between products.',
      'GLP principles regulate non-clinical safety studies supporting regulatory submissions for pharmaceuticals, chemicals, and medical devices. Study protocols define experimental design, test systems, and analytical methods before study initiation. Standard Operating Procedures (SOPs) provide detailed instructions for routine laboratory operations, ensuring method consistency and data reliability. Test system characterization demonstrates suitability of biological systems, analytical methods, and equipment for intended study purposes. Study documentation maintains complete records of all procedures, observations, and data generated during study execution, creating audit trails that enable reconstruction and verification of study conduct and results.',
      'Quality control systems form the backbone of both GMP and GLP compliance through systematic testing, monitoring, and documentation activities. Raw material and component testing verifies identity, purity, and quality characteristics before use in production or studies. In-process controls monitor critical parameters during manufacturing to ensure processes remain within specified limits. Finished product testing confirms compliance with all quality specifications before product release. Stability programs monitor product quality over time under various storage conditions to establish shelf life and storage requirements. All quality control activities must follow validated methods and maintain comprehensive documentation.',
      'Documentation requirements in GMP/GLP systems emphasize accuracy, completeness, and traceability throughout product lifecycles. Batch records capture complete manufacturing histories including materials, equipment, personnel, and process parameters for each production batch. Laboratory notebooks and electronic systems maintain detailed records of all analytical testing, including raw data, calculations, and results. Change control systems document all modifications to processes, equipment, or procedures with impact assessments and approval signatures. Deviation reports record any departures from established procedures along with investigations, root cause analyses, and corrective actions. Retention schedules preserve all quality records for specified periods to support regulatory inspections and product inquiries.',
      'Regulatory compliance and audit readiness represent ongoing responsibilities requiring continuous attention to quality system performance and regulatory requirements. Internal audit programs regularly assess GMP/GLP compliance through systematic inspections of facilities, processes, and documentation. Regulatory inspections by health authorities (FDA, EMA, etc.) verify compliance through detailed facility assessments and record reviews. Corrective and Preventive Action (CAPA) systems address identified deficiencies through root cause investigations and implementation of appropriate corrective measures. Management review ensures continuing suitability, adequacy, and effectiveness of quality systems through regular evaluation of performance metrics, audit results, and compliance status.',
    ],
    figures: [{ src: '/images/topics/gmp-glp/figure1.png', alt: 'GxP overview', caption: 'GMP/GLP concepts and common controls.' }],
  },
{
    category: 'qc',
    slug: 'documentation',
    title: 'Documentation & SOPs',
    intro: 'Good documentation practices and controlled docs.',
    paragraphs: [
      'Good Documentation Practices (GDP) represent the foundation of quality management systems in regulated industries, ensuring that all activities, decisions, and results are accurately recorded and traceable. These practices apply to all documentation types including Standard Operating Procedures (SOPs), batch records, laboratory notebooks, and electronic systems. GDP principles emphasize accuracy, completeness, consistency, and timeliness in record-keeping, creating reliable audit trails that support product quality, regulatory compliance, and continuous improvement. Proper documentation enables reconstruction of activities, verification of compliance, and protection of both organizations and personnel through clear evidence of actions taken.',
      'SOP development follows systematic processes ensuring procedures are clear, comprehensive, and effectively implemented. Document templates maintain consistent formatting including purpose, scope, responsibilities, definitions, procedures, and references sections. Step-by-step instructions provide detailed guidance that qualified personnel can follow without ambiguity or interpretation. Cross-references to related documents (forms, specifications, other SOPs) ensure complete coverage of processes without duplication. Review cycles involve subject matter experts, quality assurance personnel, and end users to verify accuracy, clarity, and practical applicability before implementation. Version control systems track document history and prevent unauthorized modifications.',
      'Controlled document management systems maintain integrity and accessibility of SOPs and other critical documents through systematic controls. Document numbering systems uniquely identify each document with version indicators and revision dates. Access controls restrict document modification to authorized personnel while allowing appropriate read access across the organization. Distribution lists ensure all relevant personnel receive current document versions and remove obsolete copies. Master files maintain complete document histories including development, review, approval, training, and revision records. Electronic document management systems provide automated controls including electronic signatures, audit trails, and change tracking capabilities.',
      'Documentation training ensures personnel understand both the content and importance of SOPs and GDP requirements. Initial training programs cover document structure, interpretation, and proper completion of record forms. Competency assessments verify understanding through practical demonstrations and knowledge testing. Refresher training addresses document revisions, updated requirements, or identified compliance issues. Training records maintain complete histories including dates, content, trainers, and assessment results for each employee. Ongoing reinforcement through supervisory observations and quality audits ensures continued compliance with documentation requirements and best practices.',
      'Record maintenance follows GDP principles ensuring accuracy, completeness, and long-term preservation of critical information. Immediate recording of activities and observations prevents memory lapses and ensures accuracy. Permanent legible ink prevents erasure or alteration of original entries, while corrections use single-line strikeouts preserving original information with initials, dates, and explanations. Date and time stamps establish chronological sequences of events and verify timely completion of activities. Signature blocks provide clear attribution of responsibility with printed names, titles, and dates. Regular record reviews verify completeness, accuracy, and compliance with established procedures before approval.',
      'Electronic documentation systems offer enhanced efficiency, accessibility, and control compared to paper-based systems while maintaining GDP compliance requirements. System validation demonstrates that electronic systems accurately perform intended functions and maintain data integrity. Access controls and permissions prevent unauthorized modifications while enabling appropriate user access. Audit trails electronically record all system activities including data creation, modifications, and deletions with user attribution and timestamps. Backup and recovery procedures protect against data loss through regular system backups and disaster recovery plans. Electronic signatures provide equivalent legal standing to handwritten signatures when properly implemented according to regulatory requirements.',
    ],
    figures: [{ src: '/images/topics/documentation/figure1.png', alt: 'Controlled document flow', caption: 'Creation → review → approval → training → revision.' }],
  },
{
    category: 'qc',
    slug: 'deviations-capa',
    title: 'Deviations & CAPA',
    intro: 'RCA, CAPA, and effectiveness checks.',
    paragraphs: [
      'Deviations and Corrective and Preventive Action (CAPA) systems represent critical quality management tools for identifying, investigating, and resolving non-conformances in regulated environments. These systematic approaches ensure that departures from established procedures or specifications are properly documented, investigated, and addressed to prevent recurrence. Effective deviation management and CAPA implementation demonstrate organizational commitment to continuous improvement, regulatory compliance, and product quality. These systems provide valuable insights into process weaknesses, training gaps, and systemic issues that might otherwise remain hidden until they cause more significant problems.',
      'Deviation identification and classification establish the foundation for appropriate investigation and response planning. Deviations occur when activities, results, or conditions depart from established procedures, specifications, or quality standards. Immediate notification procedures ensure timely reporting of deviations to appropriate personnel including quality assurance, operations management, and regulatory affairs. Risk-based classification systems categorize deviations by potential impact on product quality, safety, or regulatory compliance (critical, major, minor). This classification determines investigation depth, response urgency, and required documentation levels. Deviation logs track all identified issues, ensuring none are overlooked or improperly addressed.',
      'Deviation investigation employs systematic approaches to understand root causes and assess impacts. Investigation plans define scope, methods, responsibilities, and timelines for comprehensive analysis. Evidence collection includes document review, personnel interviews, equipment examination, and process observation to reconstruct events and identify contributing factors. Impact assessment evaluates effects on product quality, safety, efficacy, and regulatory compliance through testing, analysis, and expert judgment. Investigation reports document findings, conclusions, and recommendations with supporting evidence and rationale. Timely completion ensures that corrective actions can be implemented before additional products or processes are affected.',
      'Root Cause Analysis (RCA) methodologies identify underlying factors that enabled deviations to occur, preventing superficial fixes that allow recurrence. The "5 Whys" technique repeatedly asks "why" to drill down from symptoms to fundamental causes. Fishbone (Ishikawa) diagrams categorize potential causes across major areas (manpower, methods, machines, materials, environment, measurement) to ensure comprehensive consideration. Failure Mode and Effects Analysis (FMEA) systematically examines potential failure modes and their consequences. Pareto analysis prioritizes causes by frequency or impact to focus improvement efforts on most significant issues. Effective RCA identifies both direct causes and systemic contributing factors that enable problems to occur.',
      'CAPA implementation transforms investigation findings into concrete actions that address identified issues and prevent recurrence. Corrective actions eliminate existing problems through immediate fixes, process modifications, or system changes. Preventive actions proactively address potential issues before they occur through system improvements, training enhancements, or procedure updates. Action plans detail specific activities, responsibilities, timelines, and required resources for implementation. Progress monitoring ensures CAPA activities remain on schedule and achieve intended objectives. Documentation maintains complete records of all CAPA activities including planning, implementation, verification, and effectiveness assessment results.',
      'CAPA effectiveness verification confirms that implemented actions successfully resolved identified issues and prevented recurrence. Verification methods include process monitoring, product testing, compliance audits, and performance metric analysis to measure improvement outcomes. Follow-up periods extend sufficiently to capture normal process variations and seasonal effects that might impact effectiveness. Effectiveness reports document verification results, conclusions about CAPA success, and any additional actions required. Trend analysis monitors deviation patterns over time to identify systemic issues or recurring problems that might require broader organizational interventions. Continuous improvement cycles use CAPA data to refine processes, enhance training, and strengthen quality systems.',
    ],
    figures: [{ src: '/images/topics/deviations-capa/figure1.png', alt: 'CAPA flow', caption: 'Deviation → investigation → RCA → CAPA → effectiveness.' }],
  },
{
    category: 'qc',
    slug: 'stability',
    title: 'Stability Testing',
    intro: 'Study design, pulls, trending, and reporting.',
    paragraphs: [
      'Stability testing systematically evaluates how pharmaceutical products maintain quality attributes over time under various environmental conditions, establishing shelf life and storage requirements. These studies monitor physical, chemical, microbiological, and therapeutic properties throughout product lifecycle, ensuring safety and efficacy throughout intended use. Stability data support product registration, labeling claims, and quality control specifications while providing early detection of potential degradation problems. Regulatory requirements mandate comprehensive stability programs for all pharmaceutical products, with specific considerations for drug substances, drug products, and various dosage forms.',
      'Stability study design encompasses multiple factors ensuring data reliability and regulatory acceptance. Real-time stability studies store products under intended market conditions (typically 25°C/60%RH or 30°C/65%RH) for extended periods covering claimed shelf life. Accelerated stability studies use elevated conditions (40°C/75%RH) to rapidly identify potential degradation pathways and support tentative expiration dating. Intermediate conditions (30°C/65%RH) bridge gaps between real-time and accelerated testing for products requiring controlled room temperature storage. Stress testing under extreme conditions identifies degradation products and elucidates molecular pathways for impurity profile development and analytical method validation.',
      'Stability protocol development establishes standardized approaches for consistent study execution across products and laboratories. Protocols specify test articles, batch numbers, container types, storage conditions, testing intervals, acceptance criteria, and analytical methods. Sample size calculations ensure sufficient quantities for all planned testing throughout study duration. Testing schedules balance practical considerations with data requirements, typically including initial, 3, 6, 9, 12, 18, 24, and 36-month intervals for long-term studies. Protocol variations address specific product characteristics, packaging configurations, or regional regulatory requirements while maintaining scientific rigor and regulatory compliance.',
      'Stability testing parameters monitor all critical quality attributes that could impact product safety or efficacy throughout storage. Assay testing quantifies active pharmaceutical ingredient (API) content to ensure potency remains within specification limits. Degradation products and related substances are monitored to identify and quantify impurities that could affect safety or efficacy. Dissolution testing verifies that release characteristics remain consistent throughout shelf life. Physical parameters include appearance, hardness, friability, and particle size depending on dosage form. Microbiological testing assesses preservative efficacy and microbial limits for susceptible products. Container-closure integrity ensures packaging continues to protect contents from environmental factors.',
      'Data analysis and trending transform stability results into meaningful product understanding and shelf life determinations. Statistical regression analysis fits degradation data to linear or non-linear models, estimating degradation rates and predicting expiration dates. Trending charts display individual batch results and overall patterns across multiple production lots. Out-of-specification (OOS) investigations identify root causes for any failing results, potentially requiring corrective actions or shelf life adjustments. Shelf life calculations use statistical approaches to ensure 95% confidence that products remain within specifications throughout claimed shelf life. International Council for Harmonisation (ICH) guidelines provide regulatory framework for stability data interpretation across major markets.',
      'Stability reporting and regulatory submission package study results into comprehensive documentation supporting product approval and lifecycle management. Stability summary reports include study protocols, raw data, statistical analyses, and conclusions about product stability characteristics. Regulatory submissions incorporate stability data into Common Technical Document (CTD) format with detailed methodology and results sections. Annual stability reports update regulators on ongoing study results and confirm continued product quality throughout commercial distribution. Post-approval change management uses stability data to support manufacturing modifications, packaging changes, or shelf life extensions while maintaining regulatory compliance and product quality assurance.',
    ],
    figures: [{ src: '/images/topics/stability/figure1.png', alt: 'Stability timeline', caption: 'Study pulls over time and result trending.' }],
  },
];
